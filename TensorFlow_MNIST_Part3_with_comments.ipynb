{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTによる分類問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 17:08:57.907866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tensorflow-datasetsモジュールを使ってMNISTデータセットをインポートしていきます。まだインストールしていない場合は以下のコマンドでインストールしましょう。\n",
    "# pip install tensorflow-datasets \n",
    "# conda install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# このデータセットは、C:\\Users\\*USERNAME*\\tensorflow_datasets\\...に保存されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 17:09:11.720415: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# tfds.loadを使ってデータセットを読み込んでいきます \n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True をすることによって、データに関する情報を得ることができるようになります。\n",
    "\n",
    "# as_supervised=Trueをすることによって、データを入力とターゲットの二つののタプル形式で作成することができます \n",
    "\n",
    "# 訓練用データとテスト用データをそれぞれの変数に代入します\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# ここから検証用データセットを作っていきます。まずは、検証用データに割り当てる割合を決めます。\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# 数字を整数に変換していきます（既に整数になっている場合も念のため行います）\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# テストデータの数も変数に入れていきます\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# 数字を整数に変換していきます\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "# 元の入力データの範囲である0から255を0と1に変えていきます\n",
    "# 今回は、変換するための関数を作成していきます\n",
    "def scale(image, label):\n",
    "    # 数字が小数であることを確認します\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # 0と1の範囲におさめるため、255で割っていきます \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# mapメソッドを使ってデータの変換を行っていきます\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# テストデータに関しても同様の変換を行っていきます\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#データをシャッフルするためのバッファのサイズを決めていきます\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# シャッフルメソッドを使ってデータをシャッフルしていきます\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# takeメソッドを使ってシャッフルした検証用データを変数に代入していきます\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# skipメソッドを使って訓練用データセットを変数に代入します\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# batchの大きさを定義します\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 訓練データをバッチの数毎に分けていきます\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# テストデータに関してもバッチ毎に分けていきます\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "# バリデーションデータセットに関し、入力とターゲットそれぞれの変数を作成していきます\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
